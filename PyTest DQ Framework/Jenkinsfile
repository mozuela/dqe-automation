pipeline {
    agent any

    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }

        stage('Run Tests') {
            steps {
                withCredentials([usernamePassword(
                    credentialsId: 'postgres-dqe',
                    usernameVariable: 'DB_USER',
                    passwordVariable: 'DB_PASS'
                )]) {
                    sh '''#!/bin/bash
                    echo "=== DQE AUTOMATION ==="
                    echo "User: ${DB_USER}"
                    echo "Build: ${BUILD_NUMBER}"

                    TEST_DIR="PyTest DQ Framework"

                    if [ -d "$TEST_DIR" ]; then
                        cd "$TEST_DIR"
                        echo "Changed to test directory: $(pwd)"

                        if [ -d "../venv" ]; then
                            echo "Using existing virtual environment"
                            . ../venv/bin/activate
                        else
                            echo "Creating new virtual environment"
                            python3 -m venv ../venv
                            . ../venv/bin/activate
                        fi

                        pip install --upgrade pip

                        if [ -f "requirements.txt" ]; then
                            echo "Installing requirements from test directory"
                            pip install -r requirements.txt
                        fi

                        mkdir -p reports

                        echo "Looking for test files..."

                        # Opción 1: Buscar archivos en el directorio específico (con espacio)
                        TEST_PATH="tests/dq checks/parquet_files"
                        if [ -d "$TEST_PATH" ]; then
                            echo "Found test directory: $TEST_PATH"
                            TEST_FILES="$TEST_PATH/test_*.py"
                        else
                            echo "Test directory '$TEST_PATH' not found, searching recursively..."
                            TEST_FILES=$(find . -path "*dq checks*" -name "test_*.py" -type f | head -10 | tr '\n' ' ')
                            if [ -z "$TEST_FILES" ]; then
                                echo "Searching for any test files..."
                                TEST_FILES=$(find . -name "test_*.py" -type f | head -10 | tr '\n' ' ')
                            fi
                        fi

                        echo "Test files to run: $TEST_FILES"

                        if [ -z "$TEST_FILES" ]; then
                            echo "ERROR: No test files found"
                            exit 1
                        fi

                        echo "Running pytest from: $(which pytest)"
                        echo "Python from: $(which python)"

                        # Ejecutar pytest con los archivos encontrados
                        pytest $TEST_FILES -v \
                          --db_host="localhost" \
                          --db_port="5434" \
                          --db_name="mydatabase" \
                          --db_user="${DB_USER}" \
                          --db_password="${DB_PASS}" \
                          --parquet_path="/parquet_data" \
                          --html=reports/pytest_report.html \
                          --self-contained-html \
                          --junitxml=reports/junit_results.xml 2>&1 | tee reports/test_output.log

                        TEST_EXIT_CODE=${PIPESTATUS[0]}
                        echo "Test exit code: $TEST_EXIT_CODE"

                        exit $TEST_EXIT_CODE
                    else
                        echo "ERROR: Test directory '$TEST_DIR' not found"
                        exit 1
                    fi
                    '''
                }
            }
            post {
                always {
                    sh '''
                    echo "Copying reports to workspace root..."
                    mkdir -p reports
                    cp -r "PyTest DQ Framework/reports/"* reports/ 2>/dev/null || echo "No reports to copy"

                    echo "Workspace reports:"
                    ls -la reports/ 2>/dev/null || echo "No reports directory"
                    '''
                }
            }
        }

        stage('Generate Summary') {
            when {
                expression {
                    fileExists('reports/test_output.log')
                }
            }
            steps {
                sh '''#!/bin/bash
                echo "Analyzing test results..."

                mkdir -p reports/assets

                echo "body { font-family: Arial, sans-serif; margin: 20px; }" > reports/assets/style.css

                if [ -f "reports/test_output.log" ]; then
                    TOTAL_TESTS=$(grep -c "test_.*PASSED\\|test_.*FAILED\\|test_.*ERROR" reports/test_output.log 2>/dev/null || echo "0")
                    PASSED=$(grep -c "PASSED" reports/test_output.log 2>/dev/null || echo "0")
                    FAILED=$(grep -c "FAILED\\|ERROR" reports/test_output.log 2>/dev/null || echo "0")

                    COUNT_MISMATCH=$(grep -c -i "count mismatch" reports/test_output.log 2>/dev/null || echo "0")
                    COLUMN_NOT_FOUND=$(grep -c -i "column.*not found" reports/test_output.log 2>/dev/null || echo "0")
                    NEGATIVE_VALUES=$(grep -c -i "negative\\|below minimum" reports/test_output.log 2>/dev/null || echo "0")
                else
                    TOTAL_TESTS=0
                    PASSED=0
                    FAILED=0
                    COUNT_MISMATCH=0
                    COLUMN_NOT_FOUND=0
                    NEGATIVE_VALUES=0
                fi

                cat > reports/data_transformation_issues.md << EOF
# Data Transformation Issues Report

## Execution Information
- Date: $(date)
- Build: ${BUILD_NUMBER}
- Database User: ${DB_USER:-Not available}
- Database: localhost:5434/mydatabase

## Test Results Summary
- Total Tests Executed: $TOTAL_TESTS
- Tests Passed: $PASSED
- Tests Failed: $FAILED
EOF

                if [ $TOTAL_TESTS -gt 0 ]; then
                    SUCCESS_RATE=$(( (PASSED * 100) / TOTAL_TESTS ))
                    echo "- Success Rate: $SUCCESS_RATE%" >> reports/data_transformation_issues.md
                else
                    echo "- No tests were executed" >> reports/data_transformation_issues.md
                fi

                cat >> reports/data_transformation_issues.md << EOF

## Issues Detected in This Execution
EOF

                [ $COUNT_MISMATCH -gt 0 ] && echo "- Count Mismatch: $COUNT_MISMATCH test(s) failed due to record count discrepancies" >> reports/data_transformation_issues.md
                [ $COLUMN_NOT_FOUND -gt 0 ] && echo "- Column Structure: $COLUMN_NOT_FOUND test(s) failed due to missing columns" >> reports/data_transformation_issues.md
                [ $NEGATIVE_VALUES -gt 0 ] && echo "- Data Validation: $NEGATIVE_VALUES test(s) failed due to invalid values" >> reports/data_transformation_issues.md

                if [ $FAILED -eq 0 ] && [ $TOTAL_TESTS -gt 0 ]; then
                    echo "- No data quality issues detected" >> reports/data_transformation_issues.md
                elif [ $TOTAL_TESTS -eq 0 ]; then
                    echo "- WARNING: No tests were executed" >> reports/data_transformation_issues.md
                fi

                cat >> reports/data_transformation_issues.md << EOF

## Recommendations
1. Standardize column naming between source and target
2. Implement data reconciliation processes
3. Add validation rules for business logic
4. Monitor data quality regularly

## Framework Status
- DQE Automation: ${TOTAL_TESTS:-0} tests executed
- Test Execution: Completed
- Issue Detection: Active
- Report Generated: $(date +"%Y-%m-%d %H:%M:%S")
EOF

                echo "Summary report generated"
                '''
            }
        }

        stage('Archive Results') {
            when {
                expression {
                    fileExists('reports')
                }
            }
            steps {
                script {
                    archiveArtifacts artifacts: 'reports/**/*', fingerprint: true

                    if (fileExists('reports/pytest_report.html')) {
                        publishHTML([
                            reportDir: 'reports',
                            reportFiles: 'pytest_report.html',
                            reportName: 'DQE Test Report',
                            keepAll: true,
                            alwaysLinkToLastBuild: true
                        ])
                    }

                    if (fileExists('reports/junit_results.xml')) {
                        junit testResults: 'reports/junit_results.xml',
                               skipPublishingChecks: false,
                               allowEmptyResults: true
                    }
                }
            }
        }

        stage('Final Status') {
            steps {
                echo "=== DQE AUTOMATION COMPLETE ==="
                sh '''
                echo "Generated deliverables:"
                find reports/ -type f 2>/dev/null | sort || echo "No reports generated"
                echo ""
                echo "Check Jenkins artifacts for detailed reports"
                '''
            }
        }
    }

    post {
        always {
            echo "Pipeline finished"
            echo "Build: ${BUILD_NUMBER}"
            echo "Status: ${currentBuild.currentResult}"
            echo "Duration: ${currentBuild.durationString}"
        }
        success {
            echo "DQE Automation completed successfully"
        }
        failure {
            echo "DQE Automation failed"
        }
        unstable {
            echo "DQE Automation completed with unstable status"
        }
    }
}